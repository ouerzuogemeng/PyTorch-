### 文本处理学习过程中产生的问题，及相关问题解答整理

#### Q1: cnews_loader中的batch_iter方法，默认batch_size默认64，模型代码中验证使用了128，没见过用100，200这种随机的，batchsize选择的依据是什么？
> batchsize不是必须为2的幂次，不过一般都是按照2的幂次的方式来设定。一般来说都是解释:计算机中CPU和GPU的memory都是2进制方式存储，将batchsize设置成2的幂次可以加快二进制的计算
>> 一般来说都是解释:计算机中CPU和GPU的memory都是2进制方式存储，将batchsize设置成2的幂次可以加快二制的计算
>>> 这种底层的，其实很难说得清，不过如果按计算机组成原理的角度去思考的话，计算机在处理 乘除法的时候 效率一般低于 进位。

#### Q2: （1）对于中文情感分析，在“字”级别（counter处理）进行word2vec好还是在“词”级别（jieba处理）进行word2vec好？（2）停用词是否需要剔除？考虑到停用词里面有大量的程度副词。
> 对于某个具体的问题，可能会分到底是字符级特征比较好，还是词语级特征比较好，这个得看具体的应用。词语级是经过jieba处理后的特征，字符级是原始的特征，看你怎么提取特征的方式。
> 对于停用词是否要剔除的问题，在文本分类中，对于不同的dataset，可能不去除停用词较好(如垃圾短信分类)，但是对于某些长文本，可能去除停用词会比较好，因为那些特征是无效的

> erine号称粒度做到字，确实是可以捕捉到一些情感的变化的，比如转折词 ‘才’‘虽然’.
> 成语方面由于粒度做到了字，你会发现它容易混淆情感，所以做字训练和词训练我觉得是各有优劣的。对于成语，就不该分隔到字来分析，就应该整体训练
>> 比如差强人意，这是个褒义词，但是粒度到字以后，差这个字直接把情感拉向了负面
> 我觉得，对于填词，文本生成那类不用情感的，字训练是合适的，但是中华文化博大精深，对于要判断情感的，还是用到词来结合语境进行训练好一点

> 我觉得具体任务要具体分析。如果有些中文任务分词是不需要的，那肯定是用基于字的模型比较好，因为分词会有错误，分完词之后再应用于其他任务时会有错误累积，影响模型效果。而且分词还有新词等问题，不分词可能效果会更好的。现在类似Transformer、Bert等模型功能强大，不分词的话模型内部也可以学习到字与字之间的联系。
>> 附上张俊林博士对于用字和词的见解: https://zhuanlan.zhihu.com/p/59503959
>>>张俊林博士：至于Bert第一阶段应该采用“词输入”还是“字输入”？我个人觉得还是字输入好，我们之前在Bert放出代码，但是还没有放出预训练模型的时候，试着做过单词输入的Bert预训练模型，并和字输入的模型进行过比较，结果是词输入的效果是不如字输入的效果的。当然，预训练数据规模不是特别特别大，随着预训练数据规模的加大，可能两者的差距会减小。使用词输入，相对字输入，我觉得有几个缺点：一个是对分词工具有依赖，尤其是NER、新词等OOV问题，会比较影响模型效果。第二个是在预测的时候，如果是基于字的则预测结果标签集合较小，而如果是基于词的，明显标签空间会大很多，这很可能也会有劣势。而百度仍然采取字输入，但是Mask采取单词的方式，我觉得算是一种折中方案，包括N-gram，也算折中方案，能比较好的平衡两者，其实是挺好的。
对于中文任务来说，我觉得对于很多任务来说，分词是不必要的，随着Transformer的能力越来越强大，绝大多数任务应该以字作为输入，而连续的几个字是否应该是个单词，理论上应该让Transformer当做内部特征去学习，所以感觉中文分词是不必要存在的。当然，这个纯属个人猜想，目前无证据。

#### Q3: 标点符号在nlp中的作用大吗？我在看别人的代码的过程中看到有人将逗号，句号，引号，问号，感叹号这些符号都保存了下来，可是我觉得除了感叹号和问号，其他的标点符号似乎都没什么用?
> 个人理解：符号对于句意相近的情况下，作用比较大。否则，是word2vec的算出的向量句意差不多。这么说来，要区分差不多的语意，应该是各个标点符号都需要才对。

#### Q4: 当训练样本很多的时候，每次打开程序再次读取都要耗费很长的时间，持久化的话可否压缩占用的空间，有没有更好地节约时间的方法？
>> 数据量大是一个模糊的概念。如果数据量小于内存的大小。一次读的问题不大。只不过在代码调试阶段使用小数据调试。正式运行使用全部数据。如果数据量大于内存的大小。可以使用批量读取的方式，或者先编写一个程序把大文件分成若干份。同时，在开发中，数据库使用的越来越多。如果把数据文件提前写入数据库。则可以使用并行的方式批量处理数据。完成数据预处理等过程。这些都是可以使用的方式。

#### Q5: 训练好的模型怎么保存及重新调用？
> 只保留前10个模型。(就是每次训练后，计算 F1值，然后和之前最优的 F1 值进行比较，如果大于的话，就对该模型进行保存)

    >>>   #tensorflow设置模型储存前10
    >>>   sess = tf.Session()
    >>>   model_path = '' #保存的模型路径
    >>>   saver = tf.train.Saver(max_to_keep=10)
    >>>   saver.save(sess, model_path)
    >>>   # 模型的恢复用restore()函数
    >>>   saver.restore(sess, model_path)
  
  
    >>  #保存模型
    >>  ##仅保存权重HDF5文件
    >>  model.save_weights("model.h5") 
    >>  ##保存整个模型及结构
    >>  model.save('model_weight.h5')

    >>  #加载模型#
    >>  ##载入权重
    >>  loaded_model.load_weights("model.h5") 
    >>  ##载入整个模型结构
    >>  model = load_model('model.h5') 
    >>  https://blog.csdn.net/sjtuxx_lee/article/details/80399514

> 这个问题，这个知乎回答的不错：不错：https://www.zhihu.com/question/58287577
